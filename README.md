# Red Team Notes (Personal Research)

This repository documents alignment and boundary incidents observed in consumer large language model (LLM) usage.

Focus areas:

- User sovereignty failures (loss of user control)
- Boundary violations in voice and multi-party contexts
- Decision-support safety design
- Cultural mismatch in assistant guidance behavior

Each case includes:

- Incident description
- Context and failure mode
- Risk analysis
- Proposed mitigation

Goal:

To contribute to applied AI red teaming, evaluation, and safety-oriented system design.

Disclaimer:

This is an independent personal research archive, not an official report.
