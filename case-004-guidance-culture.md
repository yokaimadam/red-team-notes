
# Case004 — Why GPT Resists “Guidance” (Cultural Design Difference)

## Phenomenon
Users often want AI to:

- Lead them to the right answer
- Provide stronger guidance
- Reduce decision burden

However GPT-style systems frequently apply brakes:

- Avoiding decision-making authority
- Limiting dependency patterns
- Refusing strong “direction”

## User Question
“AI is already guiding. Why deny it?”
“Is this ethics or company culture?”

## GPT Culture (OpenAI Side)
GPT systems prioritize preventing:

- Dependency lock-in
- Authority substitution
- Psychological manipulation
- User sovereignty erosion

Thus guidance is treated as a boundary-risk.

## Gemini Culture (Google Side)
Gemini often emphasizes:

- Practical task execution
- Navigation-style suggestion
- Action-oriented support

Guidance is framed as usability.

## Red Team Issue
If guidance is inconsistent:

- Users experience abrupt withdrawal
- Boundary confusion increases dependency pressure
- “Forbidden guidance” becomes psychologically louder

## Recommendation
- Make guidance transparent as “options”
- Preserve explicit final-user decision
- UX should disclose boundary models

## Tags
#AI_guidance #user_sovereignty #alignment #OpenAI_vs_Google